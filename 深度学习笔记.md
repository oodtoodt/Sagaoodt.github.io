要写一份关于深度学习和神经网络的报告，但我感觉自己现在还处于向激流中探一只脚感受温度和水速的阶段。

现在能找到的比较前沿的资料实在是少。想跟上一个正在发展的学科的节奏不是一件容易的事情。网上流传的许多资料都是无意义的复制粘贴，导致很多年前的资料仍在搜索引擎上乱晃。。

那么就随便写点笔记吧...

---

神经网络的历史：

这一点已经写过一次了，关于hinton不懈的努力，关于物理与数学的交叉....

说起交叉就要提起玻尔兹曼自动机了，也就是我们这次深度信念网络的构成。

另外提一句，似乎现在rbm都没什么人用了，连google开源的TensorFlow等主流框架里都见不到rbm.......人们转而使用三十年前的东西了......

新的（2017）的说法是，「rbm虽然在数学很漂亮，但是受结构限制严重，而且在监督训练（sl）方面往往搞不过暴力反向传播。使用ReLU以及合适的初始化方法，用上CNN，搭配上强劲的GPU之后，人们发现原来的深度神经网络可以照常训练，根本不用RBM预训练。」

「说到底，相对于已经有了CNN，LSTM，以及反向传播算法8,90年代，没有本质的改变。」

「目前来看，很多对NN的贡献，都在于对NN的梯度流上」

「但是为什么当前这种方式效果很好？（全参数优化，end-to-end。形状灵活。计算高效）」

对了，另外提一句，目前最好的国象ai（除了AlphaZero之外）居然还是用的ab剪纸+启发算法以及手动调整...这个改天也可以根据dalao的报告写个笔记……

另外今天(12.15)听hinton的一个例子很有意思：有一个核电站，需要一个函数检验它的运行状态，但是我们不存在大量的训练集喂给我们的机器（也不希望有），这时候我们就需要玻尔兹曼姬来检验一个与平常不相仿的特异。

回到受限玻尔兹曼自动机。NN和玻尔兹曼分布间有着惊人的联系，比如玻尔兹曼分布可以简单的化成softmax。然后复杂运算之后还能化成sigmoid函数

概念罗列：

---

回归函数：只能稍稍理解一点：我们要把一个数据集投放在[0,1]的区间内，因为概率一定是[0,1]之间的（归一化）。而投放过之后我们在不影响数据的独立性的同时又能获得另外的一些性质方便机器学习统计....大概是这样的吧....

至于最大熵...伯努利分布，高斯分布，什么广义线性模型...最大似然估计（数理统计的作业还没捂热乎）我还没法把他们一下子都串联起来，所以先放一下。（需要补一波数学才能回来）

附属：

softmax—k分类回归：具体不知不知

sigmoid—二分类回归: 不知不知

归一化：提高梯度下降效率——抹平横轴纵轴之间的尺度差距（另外标准差标准化居然就是标准正态分布的化简）

梯度下降法：为求得极小值，使用梯度下降，需求凸，每次求偏导。

也有具体的细分。这里先不展开了。

---

Ising model—高大上的东西：不知不知

玻尔兹曼模型--受限玻尔兹曼姬来源：不知不知

深度信念网络：大概是先用一堆rbm预训练，然后再bp回调。具体……

---

Batch Normalization（BN）：

batch_size：批尺寸——具体作用不详。但是肯定对于训练有好处（现在知道应该是在批处理（Batch Normalization）期间的一个量而已）

---

反向传播(bp)：具体不知。但每一个见到的深度学习都带反向传播。每次都能反馈我可以理解，用大量训练求出每一次的导数并更新成本函数直至成本函数达到最低值，也就是对应w，b使得偏差最小的点。(这里说的是梯度下降)

对抗样本：神经网络学习到的函数不连续，只需要在图片做微小的扰动，就能让图片以很高的置信度被错误分类。

监督／无监督学习：就是说有没有给定的期望、结果值。玻尔兹曼姬就是比较典型的无监督学习

并行化运算：

向量化运算：

---

Hopfield网络：很奇怪的是我在比较古老的资料里都看得到这个（包括hinton的视频教程（2013）里），但是在比较新的（比如机器之心）一些地方则看不到这个东西....



---

CNN：

---

关于梯度流：

ReLU：

LeakyReLU：

highway：

ResNet：

噪声：

Dropout：

Batchnorm：

GRU：

GAN：

LSTM：

---

Adam优化器：

残差网络：

---

RVM（支持向量机）：



---

kaggle：

---

深度学习和神经网络的区别：（这个问题是我同学的报告被老师说是「神经网络」而非「深度学习」而引发出来的问题，但是直到现在我还是认为：我们的那个弱智课的深度学习=多层神经网络）

粘贴自不同的回答

1.原来使用bp来训练，信号是逐层更新的，当层次增加后原有的反馈信号就变得很弱，所以往往只使用一层。当网络层次增加后，我们可以使用更复杂拓扑网络结构去训练一些奇奇怪怪的东西。

2.深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。因此，“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于：

1）强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；

2）明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。

3.在多层神经网络的基础上，加入了特征学习部分。



---

12.15：第一次写该笔记。

---

12.21：第二次更新该笔记。感觉里面好多东西暂时还接触不到，层次有点高。纠结于要不要学RBM超久，就是因为看到了那句已经没什么人用RBM了……先是跟RBM息息相关的概率论不过后来发现不只是RBM，还有马尔可夫链之类的其实根本摆脱不开概率论。概率论是人类描述宇宙的最基本工具之一，也是机器学习的基础（说到底就是懒咯，疯狂为懒找理由，还需要花费大量的代价让自己找到不懒的理由...）......最后的话，为了感受hinton在RBM上寻找到的most beautiful work，还是学吧！

我还在赶弱智课的报告。

---