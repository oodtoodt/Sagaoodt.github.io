要写一份关于深度学习和神经网络的报告，但我感觉自己现在还处于向激流中探一只脚感受温度和水速的阶段。

那么就随便写点笔记吧...

神经网络的历史：

这一点已经写过一次了，关于hinton不懈的努力，关于物理与数学的交叉....
说起交叉就要提起玻尔兹曼自动机了，也就是我们这次深度信念网络的构成。

另外提一句，似乎现在rbm都没什么人用了，连google开源的TensorFlow等主流框架里都见不到rbm.......人们转而使用三十年前的东西了......

回到受限玻尔兹曼自动机。NN和玻尔兹曼分布间有着惊人的联系，比如玻尔兹曼分布可以简单的化成softmax。然后复杂运算之后还能化成sigmoid函数

概念罗列：

回归函数：只能稍稍理解一点：我们要把一个数据集投放在[0,1]的区间内，因为概率一定是[0,1]之间的（归一化）。而投放过之后我们在不影响数据的独立性的同时又能获得另外的一些性质方便机器学习统计....大概是这样的吧....
至于最大熵...伯努利分布，高斯分布，什么广义线性模型...最大似然估计（数理统计的作业还没捂热乎）我还没法把他们一下子都串联起来，所以先放一下。（需要补一波数学才能回来）
附属：
softmax—k分类回归：具体不知不知
sigmoid—二分类回归: 不知不知
归一化：提高梯度下降效率——抹平横轴纵轴之间的尺度差距（另外标准差标准化居然就是标准正态分布的化简）

梯度下降法：为求得极小值，使用梯度下降，需求凸，每次求偏导。
也有具体的细分。这里先不展开了。




Ising model—高大上的东西：不知不知

玻尔兹曼模型--受限玻尔兹曼姬来源：不知不知

batch_size：批尺寸——具体作用不详。但是肯定对于训练有好处

对了，另外提一句，目前最好的国象ai（除了AlphaZero之外）居然还是用的ab剪纸+启发算法以及手动调整...

反向传播：具体不知。但每一个见到的深度学习都带反向传播。每次都能反馈我可以理解，用大量训练求出每一次的导数并更新成本函数直至成本函数达到最低值，也就是对应w，b使得偏差最小的点。

深度信念网络：不知不知

对抗样本：神经网络学习到的函数不连续，只需要在图片做微小的扰动，就能让图片以很高的置信度被错误分类。

监督／无监督学习：

Hopfield网络：很奇怪的是我在比较古老的资料里都看得到这个（包括hinton的视频教程里），但是在比较新的（比如机器之心）一些地方则看不到这个东西....

另外今天听hinton的一个例子很有意思：有一个核电站，需要一个函数检验它的运行状态，但是我们不存在大量的训练集喂给我们的机器（也不希望有），这时候我们就需要玻尔兹曼姬来检验一个与平常不相仿的特异。

深度学习和神经网络的区别：

1.原来使用bp来训练，信号是逐层更新的，当层次增加后原有的反馈信号就变得很弱，所以往往只使用一层。当网络层次增加后，我们可以使用更复杂拓扑网络结构去训练一些奇奇怪怪的东西。

2.深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。因此，“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于：1）强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；2）明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。

3.

